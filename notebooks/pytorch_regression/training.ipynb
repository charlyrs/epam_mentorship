{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWpSa8S6kHAH"
      },
      "source": [
        "### Salary prediction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_p3JRN2kP88"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import nltk\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpADqMiRk1Gj"
      },
      "outputs": [],
      "source": [
        "# CONSTS \n",
        "BATCH_SIZE = 16\n",
        "min_count = 10\n",
        "max_count = 200000\n",
        "padding_title = 10\n",
        "padding_descr = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wyi0xmk7ld88"
      },
      "outputs": [],
      "source": [
        "!wget https://ysda-seminars.s3.eu-central-1.amazonaws.com/Train_rev1.zip\n",
        "data = pd.read_csv(\"./Train_rev1.zip\", compression='zip', index_col=None)\n",
        "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYRAU02Sljmz"
      },
      "source": [
        "Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "scu1JdI8kTRC"
      },
      "outputs": [],
      "source": [
        "text_columns = [\"Title\", \"FullDescription\"]\n",
        "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
        "TARGET_COLUMN = \"Log1pSalary\"\n",
        "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast missing values to string \"NaN\"\n",
        "\n",
        "# tokenize\n",
        "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "data['FullDescription'] = data['FullDescription'].apply(\n",
        "    lambda x: ' '.join(tokenizer.tokenize(x.lower())))\n",
        "data['Title'] = data['Title'].apply(\n",
        "    lambda x: ' '.join(tokenizer.tokenize(str(x).lower())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN57AEdmlizY"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def punct_check(text):\n",
        "  for s in string.punctuation:\n",
        "    if s in text:\n",
        "      return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gswbgrW1m3nA"
      },
      "outputs": [],
      "source": [
        "token_counts = Counter()\n",
        "\n",
        "# Count how many times does each token occur in both \"Title\" and \"FullDescription\" in total\n",
        "for describtion, title in zip(data['FullDescription'].values, data['Title'].values):\n",
        "    token_counts.update(describtion.split())\n",
        "    token_counts.update(title.split())\n",
        "\n",
        "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
        "tokens = sorted(t for t, c in token_counts.items() if c >= min_count and not punct_check(t) and c<=max_count and not t.isdigit())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCmPSjOHocpF"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader \n",
        "embeddings = gensim.downloader.load(\"glove-wiki-gigaword-100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIRUnppWpgeD",
        "outputId": "e72376c8-5846-426b-8615-f601cab64c9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>' '<unk>' '000company' '000market' '000multi' '000my' '000the'\n",
            " '00am' '00am2' '00am3']\n",
            "(33002, 100)\n"
          ]
        }
      ],
      "source": [
        "vocab_npa = np.array(tokens)\n",
        "embs_npa = np.array([embeddings.get_vector(x) if x in embeddings.vocab else np.zeros(100) for x in tokens])\n",
        "# embeddings = 0\n",
        "vocab_npa = np.insert(vocab_npa, 0, '<pad>')\n",
        "vocab_npa = np.insert(vocab_npa, 1, '<unk>')\n",
        "print(vocab_npa[:10])\n",
        "\n",
        "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\n",
        "unk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)    #embedding for '<unk>' token.\n",
        "\n",
        "#insert embeddings for pad and unk tokens at top of embs_npa.\n",
        "embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))\n",
        "print(embs_npa.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf9_V_pZrAq5"
      },
      "outputs": [],
      "source": [
        "token_to_id = {val: idx for idx, val in enumerate(vocab_npa)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhdhSUXYk_Yp"
      },
      "outputs": [],
      "source": [
        "UNK_IX, PAD_IX = map(token_to_id.get, ['<unk>', '<pad>'])\n",
        "\n",
        "def as_matrix(sequences, max_len=None):\n",
        "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
        "    if isinstance(sequences[0], str):\n",
        "        sequences = list(map(str.split, sequences))\n",
        "        \n",
        "    max_len = max_len if max_len else min(max(map(len, sequences)), max_len or float('inf'))\n",
        "    \n",
        "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
        "    for i,seq in enumerate(sequences):\n",
        "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
        "        matrix[i, :len(row_ix)] = row_ix\n",
        "    \n",
        "    return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B__xVFqalFVL",
        "outputId": "382d5388-918c-42d4-c920-c65ca442a84f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DictVectorizer(dtype=<class 'numpy.float32'>, sparse=False)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# we only consider top-1k most frequent companies to minimize memory usage\n",
        "top_companies, top_counts = zip(*Counter(data['Company']).most_common(1000))\n",
        "recognized_companies = set(top_companies)\n",
        "data[\"Company\"] = data[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
        "\n",
        "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
        "categorical_vectorizer.fit(data[categorical_columns].apply(dict, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR9bY1j2lVoZ",
        "outputId": "0a3e9465-ee77-4ef3-9597-910302338c3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size =  195814\n",
            "Validation size =  48954\n"
          ]
        }
      ],
      "source": [
        "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
        "data_train.index = range(len(data_train))\n",
        "data_val.index = range(len(data_val))\n",
        "\n",
        "print(\"Train size = \", len(data_train))\n",
        "print(\"Validation size = \", len(data_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQk9jmAql8Si"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def to_tensors(batch, device):\n",
        "    batch_tensors = dict()\n",
        "    for key, arr in batch.items():\n",
        "        if key in [\"FullDescription\", \"Title\"]:\n",
        "            batch_tensors[key] = torch.tensor(arr, device=device, dtype=torch.int64)\n",
        "        else:\n",
        "            batch_tensors[key] = torch.tensor(arr, device=device)\n",
        "    return batch_tensors\n",
        "\n",
        "\n",
        "def make_batch(data, max_len=None, word_dropout=0, device=device):\n",
        "    \"\"\"\n",
        "    Creates a keras-friendly dict from the batch data.\n",
        "    :param word_dropout: replaces token index with UNK_IX with this probability\n",
        "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
        "    \"\"\"\n",
        "    batch = {}\n",
        "    batch[\"Title\"] = as_matrix(data[\"Title\"].values, max_len[0])\n",
        "    batch[\"FullDescription\"] = as_matrix(data[\"FullDescription\"].values, max_len[1])\n",
        "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
        "    \n",
        "    if word_dropout != 0:\n",
        "        batch[\"FullDescription\"] = apply_word_dropout(batch[\"FullDescription\"], 1. - word_dropout)\n",
        "    \n",
        "    if TARGET_COLUMN in data.columns:\n",
        "        batch[TARGET_COLUMN] = data[TARGET_COLUMN].values\n",
        "    \n",
        "    return to_tensors(batch, device)\n",
        "\n",
        "def apply_word_dropout(matrix, keep_prop, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
        "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])\n",
        "    dropout_mask &= matrix != pad_ix\n",
        "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqVuPjIY0pJk"
      },
      "outputs": [],
      "source": [
        "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, device=device, **kwargs):\n",
        "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
        "    while True:\n",
        "        indices = np.arange(len(data))\n",
        "        if shuffle:\n",
        "            indices = np.random.permutation(indices)\n",
        "\n",
        "        for start in range(0, len(indices), batch_size):\n",
        "            batch = make_batch(data.iloc[indices[start : start + batch_size]], device=device, max_len = (padding_title, padding_descr), **kwargs)\n",
        "            yield batch\n",
        "        \n",
        "        if not cycle: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5sgXZP90sQL"
      },
      "outputs": [],
      "source": [
        "def print_metrics(model, data, batch_size=BATCH_SIZE, name=\"\", device=torch.device('cpu'), **kw):\n",
        "    squared_error = abs_error = num_samples = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in iterate_minibatches(data, batch_size=batch_size, shuffle=False, device=device, **kw):\n",
        "            batch_pred = model(batch)\n",
        "            squared_error += torch.sum(torch.square(batch_pred - batch[TARGET_COLUMN]))\n",
        "            abs_error += torch.sum(torch.abs(batch_pred - batch[TARGET_COLUMN]))\n",
        "            num_samples += len(batch_pred)\n",
        "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
        "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
        "    print(\"%s results:\" % (name or \"\"))\n",
        "    print(\"Mean square error: %.5f\" % mse)\n",
        "    print(\"Mean absolute error: %.5f\" % mae)\n",
        "    return mae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUZKqfXRkHAQ"
      },
      "outputs": [],
      "source": [
        "class SalaryPredictorConv(nn.Module):\n",
        "    def __init__(self, n_tokens=len(tokens), \n",
        "                 n_cat_features=len(categorical_vectorizer.vocabulary_), \n",
        "                 emb_len=100, \n",
        "                 padding_title = 10,\n",
        "                 padding_descr = 200,\n",
        "                 conv_kernel = 32):\n",
        "        super().__init__()\n",
        "        self.emb_len = emb_len\n",
        "        self.padding_title = padding_title\n",
        "        self.padding_descr = padding_descr\n",
        "        self.conv_kernel = conv_kernel\n",
        "        self.emb = nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float()).requires_grad_(True)\n",
        "        self.text = nn.Sequential(\n",
        "            nn.Conv1d(emb_len, conv_kernel*2, kernel_size=3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(conv_kernel*2, conv_kernel, kernel_size=6, padding='same'),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(conv_kernel * 3, 32),\n",
        "            nn.ReLU(), \n",
        "            nn.Linear(32, 1),\n",
        "        )\n",
        "        self.pooling_title = nn.AvgPool1d(padding_title, count_include_pad=False)\n",
        "        self.pooling_descr = nn.AvgPool1d(padding_descr, count_include_pad=False)\n",
        "        self.fc_cat = nn.Linear(n_cat_features, conv_kernel)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        descr = batch['FullDescription']\n",
        "        title = batch['Title']\n",
        "        cat = batch['Categorical']\n",
        "       \n",
        "        descr = self.emb(descr).permute(0,2,1)\n",
        "        descr = self.text(descr)\n",
        "        descr = self.pooling_descr(descr).reshape(len(descr),self.conv_kernel)\n",
        "\n",
        "        title = self.emb(title).permute(0,2,1)\n",
        "        title = self.text(title)\n",
        "        title = self.pooling_title(title).reshape(len(title),self.conv_kernel)\n",
        "\n",
        "        cat = self.relu(self.fc_cat(cat))\n",
        "\n",
        "        x = torch.cat((descr,title,cat), dim=1)\n",
        "        return self.linear(x).reshape(len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BRG0uP0Z_Dv"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.random import device_count\n",
        "class SalaryPredictorRecurent(nn.Module):\n",
        "    def __init__(self, n_tokens=len(tokens), \n",
        "                 n_cat_features=len(categorical_vectorizer.vocabulary_), \n",
        "                 emb_len=100, \n",
        "                 padding_title = 10,\n",
        "                 padding_descr = 200,\n",
        "                 rec = 32, \n",
        "                 num_layers = 2):\n",
        "        super().__init__()\n",
        "        self.emb_len = emb_len\n",
        "        self.padding_title = padding_title\n",
        "        self.padding_descr = padding_descr\n",
        "        self.rec = rec\n",
        "        self.num_layers = num_layers\n",
        "        self.emb = nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float()).requires_grad_(True)\n",
        "        self.lstm = nn.LSTM(input_size=emb_len, \n",
        "                            hidden_size=rec, \n",
        "                            num_layers=num_layers,\n",
        "                            batch_first=True, bidirectional =False )\n",
        "        self.fc_cat = nn.Linear(n_cat_features, rec)\n",
        "        self.fc1 = nn.Linear(rec * 3, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        descr = batch['FullDescription']\n",
        "        title = batch['Title']\n",
        "        cat = batch['Categorical']\n",
        "       \n",
        "        descr = self.emb(descr)\n",
        "        hidden, carry = torch.randn(self.num_layers, len(descr), self.rec), torch.randn(self.num_layers, len(descr), self.rec)\n",
        "        hidden, carry =  hidden.to(device), carry.to(device)\n",
        "        descr, _ = self.lstm(descr, (hidden, carry))\n",
        "        \n",
        "        title = self.emb(title)\n",
        "        hidden1, carry1 = torch.randn(self.num_layers, len(title), self.rec), torch.randn(self.num_layers, len(title), self.rec)\n",
        "        hidden1, carry1 =  hidden1.to(device), carry1.to(device)\n",
        "        title, _ = self.lstm(title, (hidden1, carry1))\n",
        "\n",
        "        cat = self.relu(self.fc_cat(cat))\n",
        "        \n",
        "        x = torch.cat((descr[:,-1],title[:,-1],cat), dim=1)\n",
        "        return self.fc2(self.relu(self.fc1(x))).reshape(len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbOcs_qw0rRX"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "EPOCHS = 15\n",
        "LR = 5e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-5Ibtuy0um1"
      },
      "outputs": [],
      "source": [
        "model = SalaryPredictorRecurent().to(device)\n",
        "criterion = nn.L1Loss(reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.95**epoch)\n",
        "def train(model, criterion, optimizer, scheduler, data_train):\n",
        "    prev_loss = 1000\n",
        "    patience = 2\n",
        "    trigger_times = 0\n",
        "    mae_val = []\n",
        "    mae_train = []\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"epoch: {epoch}\")\n",
        "        model.train()\n",
        "        temp = []\n",
        "        for i, batch in tqdm(enumerate(\n",
        "                iterate_minibatches(data_train, batch_size=BATCH_SIZE, device=device)),\n",
        "                total=len(data_train) // BATCH_SIZE\n",
        "            ):\n",
        "            pred = model(batch)\n",
        "            loss = criterion(pred, batch[TARGET_COLUMN])\n",
        "            temp.append(float(loss))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()\n",
        "        mae_train.append(np.mean(temp)) \n",
        "        curr_loss = print_metrics(model, data_val, device=device)\n",
        "        mae_val.append(curr_loss)\n",
        "        if curr_loss + 0.001 > prev_loss:\n",
        "            trigger_times += 1\n",
        "            print('trigger times:', trigger_times)\n",
        "            if trigger_times >= patience:\n",
        "                print('Early stopping!\\nStart to test process.')\n",
        "                break\n",
        "        else:\n",
        "            print('trigger times: 0')\n",
        "            trigger_times = 0\n",
        "            torch.save(model.state_dict(), 'model')\n",
        "            prev_loss = curr_loss"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}